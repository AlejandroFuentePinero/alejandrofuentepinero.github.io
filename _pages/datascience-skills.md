---
title: "Skills"
permalink: /datascience/skills/
layout: single
---

## Summary
Data scientist with a **research-grade modelling foundation** (spatiotemporal + hierarchical Bayesian population modelling) and **end-to-end ML delivery** (Python pipelines, evaluation, interpretability, recommender outputs). Strong scientific communication and publication record.

---

## 1) Research Practice (PhD / Publications)
- Peer-reviewed writing; methods-first documentation; reproducible reporting  
- Oral presentations (conferences, stakeholder briefings); scientific storytelling  
- Teaching and mentoring (students/collaborators; code + modelling support)  
- Project management: scoping, prioritisation, milestones, delivery under constraints  

---

## 2) Data Science Workflow
- EDA, data cleaning, missingness strategies, outliers, leakage checks, sanity checks  
- Feature engineering: encoding, scaling, transformations, selection  
- Validation design: train/val/test, cross-validation, baseline discipline, overfitting control  
- Evaluation: classification/regression metrics, calibration awareness, error analysis  
- Reproducibility: deterministic runs, configs, artefact generation, documentation  

---

## 3) Statistical, Bayesian & Spatiotemporal Modelling
- GLM/GAM foundations; mixed-effects / hierarchical modelling  
- **Hierarchical Bayesian** modelling (partial pooling, uncertainty-first reporting)  
- **Spatiotemporal** modelling mindset: structured dependence, forecasting logic, SDMs  
- Population / observation models (detectionâ€“abundance separation template)  
- **Integrated Population Models (IPMs)**  
- Bayesian tooling exposure: **Stan (Statistical Rethinking)**, **JAGS** (R-based workflows)  

---

## 4) Machine Learning (Classical)
- Supervised: linear/logistic regression, trees, random forests, gradient boosting, SVM, k-NN  
- Unsupervised: K-Means clustering, **PCA (dimensionality reduction)**, anomaly detection fundamentals  
- scikit-learn Pipelines; hyperparameter tuning; model comparison  
- Gradient boosting (incl. **XGBoost**) and practical performance tuning patterns  
- Intro exposure: **reinforcement learning** concepts (course-level foundations)  

---

## 5) Deep Learning (Foundations)
- Feedforward neural networks (classification/regression)  
- Training concepts: activation functions, loss/optimisers, regularisation, train/val monitoring  
- **TensorFlow / Keras** fundamentals (model definition, training, evaluation)  

---

## 6) NLP & Recommenders (Applied Foundations)
- Text cleaning/normalisation; tokenisation; feature inspection  
- Vectorisation: CountVectorizer, TF-IDF; Naive Bayes baselines  
- Recommender foundations: similarity metrics, collaborative filtering, constraint-aware framing  

---

## 7) Interpretability, Diagnostics & Reporting
- Feature importance; partial dependence style diagnostics  
- SHAP-style global/local explanation workflows (where appropriate)  
- Model debugging: residual/error slicing, failure modes, robustness checks  
- Decision-ready reporting (clear tradeoffs, assumptions, limitations)  

---

## 8) Programming & Libraries
- **Python (primary):** pandas, NumPy, scikit-learn, matplotlib, seaborn, plotly; regex; logging; OOP  
- **R:** tidyverse, ggplot2; sf/terra; Shiny familiarity  
- **SQL:** PostgreSQL + **pgAdmin4** (joins, CTEs, window functions, views, NULL-safe patterns)  

---

## 9) Data Engineering, I/O & Integration
- Data ingestion/processing: CSV/Excel/text; file I/O; schema discipline  
- Web data basics: requests + BeautifulSoup; PDF parsing basics  
- Integration patterns: API + SQL workflows; joining complex relational datasets  
- Exposure to distributed-data tooling: **Spark (intro)**  

---

## 10) Software Engineering & Reproducibility
- Git/GitHub: branching/merging, PR workflow, conflict resolution, diff-driven review  
- Environments: venv/conda (Python), renv (R)  
- Code quality: assertions/validation utilities, docstrings, type hints where useful, pytest-style testing patterns  
- Modular architecture: reusable modules, script-to-pipeline conversion, clean interfaces  
- Reproducible reporting: R Markdown; Jupyter Notebook; artefact generation + methods-first documentation  

---

## 11) Databases & Storage
- PostgreSQL setup and querying; analytics-oriented schema reasoning  
- Microsoft Access (legacy support)  

---

## 12) Visualisation & Dashboards
- EDA + diagnostic plots in Python/R  
- Interactive outputs: Streamlit (Python), Shiny familiarity (R), Plotly  

---

## 13) Geospatial & Remote Sensing
- **GIS / remote sensing** workflows: raster/vector, spatial joins, geoprocessing pipelines  
- Spatial feature engineering; canopy/landscape metrics  
- Parallelised spatial workflows where appropriate  

---

## 14) LLMs, Prompt Engineering & Agents (Foundations)
- Prompt engineering for summarisation, classification, transformation, generation  
- Reliability patterns: structured prompts, iterative refinement, evaluation loops  
- Agentic patterns: planning/acting/reflection loops, tool calling, retrieval, multi-agent coordination  
- Graph-style orchestration concepts (LangGraph): state, control flow, tracing/debugging, retries, HITL stages  
- API exposure: **OpenAI API** basics (prompt-driven app patterns)  

---

## Education & Training
A detailed, certificate-linked list of formal education and courses is maintained here: **/datascience/education/**
