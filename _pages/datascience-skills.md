---
title: "Skills"
permalink: /datascience/skills/
layout: archive
author_profile: true
classes: wide projects-page
---

<figure style="text-align:center; margin:0;">
  <img
    src="https://raw.githubusercontent.com/AlejandroFuentePinero/alejandrofuentepinero.github.io/master/files/skill_summary.png"
    alt="Skills at a Glance"
    style="width:100%; max-width:1100px; height:auto; display:block; margin:0 auto;"
  >
</figure>


## What I’m best at

I drive modelling work from definition to delivery: setting objectives and success metrics, building the data foundation, developing features and models, and shipping reproducible outputs that inform decisions. My differentiator is deep statistical judgement and uncertainty-aware inference, with particular strength in hierarchical and Bayesian modelling, producing results that remain robust under noise, sparse data, selection bias, and spatiotemporal structure.

---

## What I deliver

- Full-cycle modelling delivery: define objectives and success metrics, design the data foundation, engineer features, train models, validate rigorously, and deliver reproducible outputs that support decisions.
- Model selection with statistical judgement: select model families aligned to the data-generating process, including structured dependence across space and time.
- Bayesian hierarchical inference: partial pooling and principled uncertainty propagation for robust estimates and uncertainty-aware decision-making under sparse, noisy, or biased data.
- Experimentation and causal reasoning: A/B testing fundamentals, power and effect-size framing, and clear treatment of confounding, selection bias, and the limits of identification.
- Reliable evaluation and communication: leakage checks, calibration awareness, error slicing, robustness and stress testing, and transparent reporting of assumptions, tradeoffs, and limitations.

---

## Core stack

**Tools**
- **Python:** pandas, NumPy, scikit-learn; pipelines; visualisation (matplotlib/plotly)
- **SQL (PostgreSQL):** joins, CTEs, window functions; analytics transformations
- **Software engineering:** Git/GitHub; modular code; testing/validation (pytest patterns); reproducible environments (conda/venv)

**Core methods**
- **Bayesian & hierarchical modelling:** partial pooling, uncertainty quantification and propagation  
- **Machine learning:** supervised/unsupervised, model selection, tuning, gradient boosting (incl. XGBoost)  
- **Experimentation & causal:** A/B testing, power/effect size, confounding, selection bias  
- **Evaluation discipline:** leakage, calibration, error analysis, robustness  

--- 

## Academia transferability

- **First-principles problem solving:** break down open-ended questions, formalise hypotheses, and select methods aligned to the data-generating process.  
- **Study and measurement design:** design sampling and measurement protocols, define data quality standards, and handle bias, missingness, and uncertainty at the source.  
- **Full-cycle project delivery:** scope work, set milestones, manage tradeoffs, and deliver high-quality outcomes under real constraints.  
- **High-standard technical communication:** publish peer-reviewed work and produce clear, auditable narratives with explicit assumptions, evidence, and limitations.  
- **Cross-functional collaboration and mentorship:** work across disciplines, mentor and upskill collaborators, and drive alignment through proposal-style writing and resourcing justification (grants/funding).  

---

## Technical depth

<details>
<summary><strong>Bayesian, hierarchical & spatiotemporal modelling</strong></summary>

<div markdown="1">

- Generalised linear models (GLMs) and generalised additive models (GAMs) for nonlinear effects  
- Threshold and segmented regression for decision-point inference  
- Hierarchical and mixed-effects modelling; partial pooling  
- Bayesian inference with uncertainty quantification and propagation; priors as explicit assumptions  
- Spatiotemporal modelling: structured dependence, forecasting logic, species distribution modelling (SDMs)  
- Observation vs process modelling: detection–abundance separation; N-mixture models  
- Integrated Population Models (IPMs)

</div>
</details>

<details>
<summary><strong>Machine learning (classical)</strong></summary>

<div markdown="1">

- Supervised learning: linear/logistic regression, tree-based models, random forests, gradient boosting (incl. XGBoost), SVM, k-NN  
- Unsupervised learning: PCA, clustering (K-Means), anomaly detection  
- scikit-learn Pipelines; hyperparameter tuning; model comparison and baselines  

</div>
</details>

<details>
<summary><strong>Deep learning</strong></summary>

<div markdown="1">

- Neural networks for classification and regression  
- Training fundamentals: loss functions, optimisers, regularisation, monitoring and early stopping  
- TensorFlow / Keras: model definition, training, evaluation  

</div>
</details>

<details>
<summary><strong>Evaluation, interpretability & reporting</strong></summary>

<div markdown="1">

- Validation design: train/val/test, cross-validation, temporal/blocked splits where appropriate  
- Evaluation discipline: leakage checks, calibration awareness, error analysis and slicing, robustness/stress testing  
- Interpretability: feature importance, partial dependence, SHAP-style global/local explanations  
- Decision-ready reporting: assumptions, limitations, tradeoffs, and clear recommendations  

</div>
</details>

<details>
<summary><strong>Experimentation & causal inference</strong></summary>

<div markdown="1">

- A/B testing fundamentals: hypotheses, metrics, power and effect size  
- Causal inference basics: confounding, selection bias, counterfactual framing, limits of identification  
- Practical decision-making under uncertainty: interpreting results and communicating tradeoffs  

</div>
</details>

<details>
<summary><strong>Data engineering & integration</strong></summary>

<div markdown="1">

- Data ingestion and transformation: structured files, schema discipline, reliable I/O  
- SQL-centric data work: joins across complex relational datasets, analytics transformations  
- API integration patterns: extracting, normalising, and joining external data sources  

</div>
</details>

<details>
<summary><strong>Software engineering & reproducibility</strong></summary>

<div markdown="1">

- Git/GitHub workflows: branching, pull requests, code review, merge discipline  
- Maintainable codebases: modular architecture, clean interfaces, reusable components, pipeline-style structure  
- Quality controls: input validation, assertions, unit tests (pytest patterns), docstrings, type hints where useful  
- Reproducibility: environment management (conda/venv), deterministic runs, versioned artefacts, methods-first documentation  

</div>
</details>

<details>
<summary><strong>NLP, recommenders & text features</strong></summary>

<div markdown="1">

- Text preprocessing and inspection: normalisation, tokenisation, feature auditing  
- Vectorisation: bag-of-words and TF-IDF; baseline classifiers (Naive Bayes)  
- Recommender foundations: similarity metrics, collaborative filtering, constraint-aware framing  

</div>
</details>

<details>
<summary><strong>LLMs, prompt engineering & agents</strong></summary>

<div markdown="1">

- Prompting for structured outputs; reliability patterns (prompt scaffolds, self-checks, evaluation loops)  
- Tool calling and retrieval patterns; schema/contract design for model outputs  
- Agent workflows: planning/acting loops, orchestration, retries, human-in-the-loop checkpoints  
- LangGraph concepts: state, control flow, tracing/debugging  
- OpenAI API integration patterns for prompt-driven applications  

</div>
</details>

<details>
<summary><strong>Geospatial & remote sensing</strong></summary>

<div markdown="1">

- Raster/vector workflows; spatial joins; geoprocessing pipelines  
- Spatial feature engineering; landscape/canopy metrics  
- Scalable spatial processing  

</div>
</details>

<details>
<summary><strong>Visualisation & lightweight apps</strong></summary>

<div markdown="1">

- Visualisation: matplotlib, seaborn, plotly; ggplot2  
- Lightweight apps: Streamlit (Python), Shiny (R)  

</div>
</details>

---

## Education & Training
A detailed, certificate-linked list of formal education and courses is maintained here: **[Education & Training](/datascience/education/)**.
